{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Datachecks","text":"<p>Welcome to the Datachecks Documentation!</p> <p>Let's jump to the Getting Started!</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>You can easily launch this example in just 5 minutes.</p>"},{"location":"getting_started/#installation","title":"Installation","text":""},{"location":"getting_started/#mac-os-and-linux","title":"MAC OS and Linux","text":"<p>Install Datachecks using the pip package manager. Below we are installing the package with the postgres extra, which is required for this example.</p> <pre><code>pip install 'datachecks[postgres]' -U\n</code></pre>"},{"location":"getting_started/#quick-setup-of-database-test-data","title":"Quick Setup of Database &amp; Test Data","text":"<p>Ignore if you already have a PostgreSql setup</p> Create a SQL file <p>Create a sql file named <code>init.sql</code> with the following contents: init.sql<pre><code>CREATE TABLE IF NOT EXISTS products (\n    id INTEGER PRIMARY KEY,\n    name TEXT,\n    category TEXT,\n    country_code TEXT,\n    price INTEGER\n);\nINSERT INTO products VALUES\n    (1, 'Apple', 'Fruit', 'IN', 100),\n    (2, 'Orange', 'Fruit', 'IN', 80),\n    (3, 'Banana', 'Fruit', 'IN', 50),\n    (4, 'Mango', 'Fruit', 'IN', 150),\n    (5, 'Pineapple', 'Fruit', 'IN', 200),\n    (6, 'Papaya', 'Fruit', 'IN', 100),\n    (7, 'Grapes', 'Fruit', 'IN', 120),\n    (8, 'Strawberry', 'Fruit', 'IN', 300),\n    (9, 'Kiwi', 'Fruit', 'US', 200),\n    (10, 'Watermelon', 'Fruit', 'US', 100);\n</code></pre></p> Postgres Docker Compose file <p>Create a <code>docker-compose.yml</code> for postgres:</p> docker-compose.yaml<pre><code>version: '3'\nservices:\n  dcs-demo-postgres:\n    container_name: dcs-demo-postgres\n    image: postgres\n    environment:\n      POSTGRES_DB: dcs_demo\n      POSTGRES_USER: dbuser\n      POSTGRES_PASSWORD: dbpass\n      PGDATA: /data/postgres\n    volumes:\n      - dcs-demo-postgres:/data/postgres\n      - ./init.sql:/docker-entrypoint-initdb.d/init.sql\n    ports:\n      - \"5431:5432\"\n    networks:\n      - dcs-demo-postgres\n    restart: unless-stopped\n\nnetworks:\n  dcs-demo-postgres:\n    driver: bridge\n\nvolumes:\n  dcs-demo-postgres:\n    driver: local\n</code></pre>"},{"location":"getting_started/#datachecks-configuration-file","title":"Datachecks Configuration File","text":"<p>Create a configuration file <code>dcs_config.yaml</code> with the following contents:</p> dcs_config.yaml<pre><code>data_sources:\n  - name: product_db\n    type: postgres\n    connection:\n      host: 127.0.0.1\n      port: 5431\n      username: dbuser\n      password: dbpass\n      database: dcs_demo\nvalidations for product_db.products:\n  - count_of_products:\n      on: count_rows\n      threshold: \"&gt; 0 &amp; &lt; 1000\"\n  - max_product_price_in_india:\n      on: max(price)\n      where: \"country_code = 'IN'\"\n      threshold: \"&lt; 190\"\n</code></pre>"},{"location":"getting_started/#run-datachecks","title":"Run Datachecks","text":"<p>Datachecks can be run in two ways using the CLI or the Python API.</p>"},{"location":"getting_started/#run-datachecks-in-cli","title":"Run Datachecks in CLI","text":"<pre><code>datachecks inspect --config-path ./dcs_config.yaml\n</code></pre> <p>While running the above command, you should see the following output:</p> <p></p>"},{"location":"getting_started/#generate-metrics-validation-report","title":"Generate Metrics Validation Report","text":"<p>You can generate a beautiful data quality report with all the metrics with just one command. This html report can be shared with the team.</p> <p><pre><code>datachecks inspect --config-path ./dcs_config.yaml --html-report\n</code></pre> </p>"},{"location":"getting_started/#run-datachecks-in-python","title":"Run Datachecks in Python","text":"<pre><code>from datachecks.core import Inspect\n\n\nif __name__ == \"__main__\":\n    inspect = Inspect()\n    inspect.add_configuration_yaml_file(\"dcs_config.yaml\")\n    inspect_output = inspect.run()\n    print(inspect_output.metrics)\n    # User the metrics to send or store somewhere\n    # It can be sent to elk or any time series database\n</code></pre>"},{"location":"configuration/datasource_configuration/","title":"Data Source Configuration","text":"<p>Datachecks will read datasource configuration under the key <code>datasources</code> in the configuration file. User can define multiple datasource in the configuration file under <code>datasources</code> key.</p> <p>For example:</p> <pre><code>data_sources:\n  - name: product_db\n    type: postgres\n    connection:\n      host: 127.0.0.1\n      port: 5421\n      username: !ENV ${DB1_USER}\n      password: !ENV ${DB1_PASS}\n      database: dcs_db\n</code></pre>"},{"location":"configuration/datasource_configuration/#environment-variables","title":"Environment Variables","text":"<p>Datachecks supports environment variables in the configuration file. Environment variables can be used in the configuration file using the syntax <code>!ENV ${ENV_VARIABLE}</code>. For example:</p> <pre><code>data_sources:\n  - name: product_db\n    type: postgres\n    connection:\n      host: !ENV ${DB_HOST}\n</code></pre>"},{"location":"configuration/datasource_configuration/#configuration-details","title":"Configuration Details","text":"Parameter Mandatory Description <code>name</code> The name of the datasource. The name should be unique. <code>type</code> The type of the datasource. Possible values are <code>postgres</code>, <code>opensearch</code> etc. Type of datasource mentioned in each supported datasource documentation <code>connection</code> The connection details of the datasource. The connection details are different for each datasource. The connection details are mentioned in each supported datasource documentation."},{"location":"configuration/metric_configuration/","title":"Metric Configuration","text":"<p>Datachecks will read metrics configuration under the key <code>metrics</code> in the configuration file. User can define multiple metrics in the configuration file under <code>metrics</code> key.</p> <p>For example:</p> <pre><code>validations for mysql_db.table_name:\n  - freshness_example:\n      on: freshness(last_updated)\n      threshold: \"&gt; 86400\" ##Freshness metric value is in seconds. Validation error if metric value is greater than 86400 seconds.\n</code></pre>"},{"location":"configuration/metric_configuration/#configuration-details","title":"Configuration Details","text":"Parameter Mandatory Description <code>&lt;key filed&gt;</code> The name of the validation. The name should be unique. <code>on</code> The type of the validation function. Possible values are <code>freshness</code>, <code>row_count</code> etc. Type of validation mentioned in every metric documentation <code>where</code> The where filter to be applied on the filed. In <code>where</code> field we can pass <code>SQL Query</code>(In ase of SQl DB) or <code>Search Query</code>(In ase of search engine). For example:  <code>where: city = 'bangalore' AND age &gt;= 30</code> <code>threshold</code> The validation will be applied on the validation value. A validation error will be invoked if the metric value violate threshold value.  Possible values for threshold are <code>&gt;</code>, <code>&gt;=</code>, <code>=</code> , <code>&lt;</code>, <code>&lt;=</code>. We can combine multiple operators   For example:  <code>threshold: \"&gt;= 10 &amp; &lt;= 100\"</code>"},{"location":"configuration/metric_configuration/#validation-types","title":"Validation Types","text":"<p>Supported Validation functions are</p> Validation Group Validation Type Reliability Freshness Reliability Row Count Reliability Document Count Numeric Distribution Average Numeric Distribution Minimum Numeric Distribution Maximum Numeric Distribution Sum Numeric Distribution Variance Numeric Distribution Standard Deviation Uniqueness Distinct Count Uniqueness Duplicate Count Completeness Null Count Completeness Null Percentage Completeness Empty Count Completeness Empty Percentage Special Custom SQL Validity Count UUID Validity Percentage UUID"},{"location":"integrations/bigquery/","title":"GCP BigQuery","text":""},{"location":"integrations/bigquery/#install-dependencies","title":"Install Dependencies","text":"<pre><code>pip install datachecks[bigquery]\n</code></pre>"},{"location":"integrations/bigquery/#define-datasource-connection-in-configuration-file","title":"Define DataSource Connection in Configuration File","text":"<p>BigQuery datasource can be defined as below in the config file.</p> <p>The type of the data source must be <code>bigquery</code>.</p> <p>Below is an example of the configuration file.</p> <pre><code>data_sources:\n  - name: bigquery_datasource\n    type: bigquery\n    config:\n      project: &lt;gcp_project_id&gt;\n      dataset: &lt;gcp_dataset_name&gt;\n      credentials_base64: &lt;base64 encoded credentials json&gt;\n</code></pre>"},{"location":"integrations/bigquery/#how-to-create-base64-encoded-credentials-json","title":"How to create base64 encoded credentials json?","text":"<p>To create the base64 encoded string you can use the command line tool <code>base64</code>, or <code>openssl base64</code>, or <code>python -m base64</code></p>"},{"location":"integrations/databricks/","title":"Databricks","text":""},{"location":"integrations/databricks/#install-dependencies","title":"Install Dependencies","text":"<pre><code>pip install datachecks[databricks]\n</code></pre>"},{"location":"integrations/databricks/#define-datasource-connection-in-configuration-file","title":"Define DataSource Connection in Configuration File","text":"<p>Databricks datasource can be defined as below in the config file.</p> <p>The type of the data source must be <code>databricks</code>.</p> <p>Below is an example of the configuration file.</p> <pre><code>data_sources:\n  - name: databricks_datasource\n    type: databricks\n    config:\n      host: \"&lt;&gt;.cloud.databricks.com\"\n      port: 443\n      schema: \"test_schema\"\n      catalog: \"test_catalog\"\n      http_path: \"sql/1.0/warehouses/...\"\n      token: \"36 char token generated in Databricks\"\n</code></pre>"},{"location":"integrations/elasticsearch/","title":"ElasticSearch","text":""},{"location":"integrations/elasticsearch/#install-dependencies","title":"Install Dependencies","text":"<pre><code>pip install datachecks[elasticsearch]\n</code></pre>"},{"location":"integrations/elasticsearch/#define-datasource-connection-in-configuration-file","title":"Define DataSource Connection in Configuration File","text":"<p>ElasticSearch data source can be defined as below in the config file.</p> <p>The type of the data source must be <code>elasticsearch</code>.</p> <p>Below is an example of the configuration file.</p> <pre><code>data_sources:\n  - name: elasticsearch_datasource\n    type: elasticsearch\n    config:\n      host: localhost\n      port: 9200\n      username: admin|optional\n      password: changeme|optional\n</code></pre>"},{"location":"integrations/mssql/","title":"MS SQL Server","text":""},{"location":"integrations/mssql/#install-dependencies","title":"Install Dependencies","text":"<pre><code>pip install datachecks[mssql]\n</code></pre>"},{"location":"integrations/mssql/#define-datasource-connection-in-configuration-file","title":"Define DataSource Connection in Configuration File","text":"<p>MS SQL Server data source can be defined as below in the config file.</p> <pre><code># config.yaml\ndata_sources:\n  - name: mssql_datasource\n    type: mssql\n    config:\n      host: test.database.windows.net\n      port: 1433\n      username: dbuser\n      password: DBpass123\n      database: test-dcs\n      schema: dbo\n      driver: ODBC Driver 17 for SQL Server\n</code></pre>"},{"location":"integrations/mysql/","title":"Mysql","text":""},{"location":"integrations/mysql/#install-dependencies","title":"Install Dependencies","text":"<pre><code>pip install datachecks[mysql]\n</code></pre>"},{"location":"integrations/mysql/#create-a-user","title":"Create a user","text":"<p>As a super-user, please execute the following SQL commands in order to create a new group, assign a user to that group, and grant necessary permissions to access and monitor system tables.</p> <p>Please ensure that a secure password is generated and stored properly as it will be used for adding datasource in configuration file</p> <pre><code>CREATE ROLE dcs_role;\nGRANT REFERENCES ON *.* TO dcs_role;\nGRANT USAGE ON *.* TO dcs_role;\nGRANT SELECT ON *.* TO dcs_role;\n\nCREATE USER dcs_user IDENTIFIED BY 'DBpass123';\n\nGRANT dcs_role to dcs_user WITH ADMIN OPTION;\nSET DEFAULT ROLE dcs_role TO dcs_user;\n</code></pre>"},{"location":"integrations/mysql/#granting-permissions-to-tables-in-a-schema","title":"Granting permissions to tables in a schema","text":"<p>For each schema, execute the following three commands to grant read-only access. Below is the example for granting access to the public schema.</p> <pre><code>-- Grant usage on schema and select on current and future child tables\nGRANT USAGE ON schema_name.* TO dcs_role;\nGRANT SELECT ON schema_name.* TO dcs_role;\nGRANT ALL PRIVILEGES ON schema_name.* TO dcs_role;\n</code></pre>"},{"location":"integrations/mysql/#define-datasource-connection-in-configuration-file","title":"Define DataSource Connection in Configuration File","text":"<p>Mysql data source can be defined as below in the config file.</p> <pre><code># config.yaml\ndata_sources:\n  - name: mysql_datasource\n    type: mysql\n    config:\n      host: localhost\n      port: 3306\n      user: dbuser\n      password: DBpass123\n      database: dc_db\n</code></pre>"},{"location":"integrations/opensearch/","title":"OpenSearch","text":""},{"location":"integrations/opensearch/#install-dependencies","title":"Install Dependencies","text":"<pre><code>pip install datachecks[opensearch]\n</code></pre>"},{"location":"integrations/opensearch/#define-datasource-connection-in-configuration-file","title":"Define DataSource Connection in Configuration File","text":"<p>OpenSearch data source can be defined as below in the config file.</p> <p>The type of the data source must be <code>opensearch</code>.</p> <p>Below is an example of the configuration file.</p> <pre><code>data_sources:\n  - name: opensearch_datasource\n    type: opensearch\n    config:\n      host: localhost\n      port: 9200\n      username: admin\n      password: changeme\n</code></pre>"},{"location":"integrations/postgres/","title":"PostgreSQL","text":""},{"location":"integrations/postgres/#install-dependencies","title":"Install Dependencies","text":"<pre><code>pip install datachecks[postgres]\n</code></pre>"},{"location":"integrations/postgres/#create-a-user","title":"Create a user","text":"<p>As a super-user, please execute the following SQL commands in order to create a new group, assign a user to that group, and grant necessary permissions to access and monitor system tables.</p> <p>Please ensure that a secure password is generated and stored properly as it will be used for adding datasource in configuration file</p> <pre><code>-- Create user and group\nCREATE USER dcs_user WITH PASSWORD 'DBpass123';\n\nCREATE GROUP dcs_group;\n\nALTER GROUP dcs_group ADD USER dcs_user;\n\n-- Grant Postgres' monitor role to the dcs_group\nGRANT pg_monitor TO dcs_group\n</code></pre>"},{"location":"integrations/postgres/#granting-permissions-to-tables-in-a-schema","title":"Granting permissions to tables in a schema","text":"<p>For each schema, execute the following three commands to grant read-only access. Below is the example for granting access to the public schema.</p> <pre><code>-- Grant all permissions to the dcs_group\nGRANT USAGE ON SCHEMA \"public\" TO GROUP dcs_group;\n\nGRANT SELECT ON ALL TABLES IN SCHEMA \"public\" TO GROUP dcs_group;\n\nALTER DEFAULT PRIVILEGES IN SCHEMA \"public\" GRANT SELECT ON TABLES TO GROUP dcs_group;\n</code></pre>"},{"location":"integrations/postgres/#define-datasource-connection-in-configuration-file","title":"Define DataSource Connection in Configuration File","text":"<p>Postgresql data source can be defined as below in the config file.</p> <pre><code># config.yaml\ndata_sources:\n  - name: postgres_datasource\n    type: postgres\n    config:\n      host: localhost\n      port: 5432\n      user: dbuser\n      password: DBpass123\n      database: postgres\n      schema: public\n</code></pre>"},{"location":"integrations/redshift/","title":"AWS Redshift","text":""},{"location":"integrations/redshift/#install-dependencies","title":"Install Dependencies","text":"<pre><code>pip install datachecks[redshift]\n</code></pre>"},{"location":"integrations/redshift/#define-datasource-connection-in-configuration-file","title":"Define DataSource Connection in Configuration File","text":"<p>AWS Redshift datasource can be defined as below in the config file.</p> <p>The type of the data source must be <code>redshift</code>.</p> <p>Below is an example of the configuration file.</p> <pre><code>data_sources:\n  - name: redshift_datasource\n    type: redshift\n    config:\n      host: &lt;redshift_host&gt;.&lt;region&gt;.redshift.amazonaws.com\n      port: &lt;redshift_port&gt;\n      username: &lt;redshift_username&gt;\n      password: &lt;redshift_password&gt;\n      database: &lt;redshift_database&gt;\n</code></pre>"},{"location":"integrations/snowflake/","title":"GCP BigQuery","text":""},{"location":"integrations/snowflake/#install-dependencies","title":"Install Dependencies","text":"<pre><code>pip install datachecks[snowflake]\n</code></pre>"},{"location":"integrations/snowflake/#define-datasource-connection-in-configuration-file","title":"Define DataSource Connection in Configuration File","text":"<p>Snowflake datasource can be defined as below in the config file.</p> <p>The type of the data source must be <code>snowflake</code>. Note: Ensure sure that the account follows the correct format for the region. For more information, see Snowflake documentation.</p> <p>Below is an example of the configuration file.</p> <pre><code>data_sources:\n  - name: snowflake_datasource\n    type: snowflake\n    config:\n      account: &lt;snowflake account id&gt;\n      username: &lt;snowflake username&gt;\n      password: &lt;snowflake password&gt;\n      database: &lt;snowflake database name&gt;\n      schema: &lt;snowflake schema name&gt;\n      warehouse: &lt;snowflake warehouse name&gt;\n      role: &lt;snowflake role name&gt;\n</code></pre>"},{"location":"support/contact/","title":"Contact","text":""},{"location":"support/contact/#github","title":"Github","text":"<p>Open an issue on GitHub to report bugs and ask questions.</p>"},{"location":"support/contact/#slack-community","title":"Slack Community","text":"<p>Please join our Slack Channel to chat and connect.</p>"},{"location":"support/contact/#email","title":"Email","text":"<p>Drop a mail at hi@waterdip.ai, and we will get back to you.</p>"},{"location":"support/usage_analytics/","title":"Telemetry","text":""},{"location":"support/usage_analytics/#what-is-telemetry","title":"What is Telemetry?","text":"<p>Telemetry refers to the collection of usage data. We collect some data to understand how many users we have and how they interact with Datachecks. This helps us improve the tool and prioritize implementing the new features. Below we describe what is collected, how to opt out and why we'd appreciate if you keep the telemetry on.</p>"},{"location":"support/usage_analytics/#what-data-is-collected","title":"What data is collected?","text":"<p>Datachecks collects anonymous usage data to help our team improve the tool and to apply development efforts to where our users need them most.</p> <p>We capture one event, when the inspect run is finished. No user data or potentially sensitive information is or ever will be collected. The captured data is limited to:</p> <ul> <li>Operating System and Python version</li> <li>Number of metrics generated</li> <li>Error message, if any, truncated to the first 20 characters.</li> </ul>"},{"location":"support/usage_analytics/#how-to-enabledisable-telemetry","title":"How to enable/disable telemetry?","text":"<p>By default, telemetry is enabled. To disable the data collection you can Set environment variable <code>DISABLE_DCS_ANONYMOUS_TELEMETRY</code> to <code>True</code></p>"},{"location":"support/usage_analytics/#should-i-opt-out","title":"Should I opt out?","text":"<p>Being open-source, we have no visibility into the tool usage unless someone actively reaches out to us or opens a GitHub issue. We\u2019d be grateful if you keep the telemetry on since it helps us answer questions like:</p> <ul> <li>How many people are actively using the tool?</li> <li>Which features are being used most?</li> <li>What is the environment you run Datachecks in?</li> </ul> <p>It helps us prioritize the development of new features and make sure we test the performance in the most popular environments.</p> <p>We understand that you might still prefer not to share any telemetry data, and we respect this wish. Follow the steps above to disable the data collection.</p>"},{"location":"validations/combined/","title":"Combined Metrics","text":"<p>Combined metrics in data quality serve as a cornerstone for ensuring the accuracy and efficiency of your data operations. These metrics provide a holistic view of your data ecosystem, amalgamating various aspects to paint a comprehensive picture.</p> <p>By consistently tracking these combined metrics, you gain invaluable insights into the overall performance of your data infrastructure. This data-driven approach enables you to make informed decisions on optimization, resource allocation, and system enhancements. Moreover, these metrics act as sentinels, promptly detecting anomalies or bottlenecks within your data pipelines. This proactive stance allows you to mitigate potential issues before they escalate, safeguarding the integrity of your data.</p>"},{"location":"validations/combined/#available-function","title":"Available Function","text":"<ul> <li><code>div()</code></li> <li><code>sum()</code></li> <li><code>mul()</code></li> <li><code>sub()</code></li> </ul>"},{"location":"validations/combined/#single-function-expression","title":"Single Function Expression","text":"<p>Example</p> dcs_config.yaml<pre><code>metrics:\n- name: combined_metric_example\n  metric_type: combined\n  expression: sum(count_product_invalid, count_product_valid)\n</code></pre>"},{"location":"validations/combined/#multiple-functions-expression","title":"Multiple Functions Expression","text":"<p>We can combine multiple functions in one expression.</p> <p>Example</p> dcs_config.yaml<pre><code>metrics:\n- name: combined_metric_example\n  metric_type: combined\n  expression: div(sum(count_product_invalid, count_product_valid), count_product)\n</code></pre>"},{"location":"validations/combined/#multiple-functions-expression-with-numeric-values","title":"Multiple Functions Expression with numeric values","text":"<p>We can combine multiple functions in one expression with numeric values.</p> <p>Example</p> dcs_config.yaml<pre><code>metrics:\n- name: combined_metric_example\n  metric_type: combined\n  expression: div(sum(count_product_invalid, 10), count_product)\n</code></pre>"},{"location":"validations/combined/#limitations","title":"Limitations","text":"<p>Combined metrics accepts only 2 arguments in one functions.</p>"},{"location":"validations/completeness/","title":"Completeness Validations","text":"<p>Completeness Validations play a crucial role in data quality assessment, ensuring your datasets are comprehensive and reliable. By regularly monitoring these validations, you can gain profound insights into the extent to which your data captures the entirety of the intended information. This empowers you to make informed decisions about data integrity and take corrective actions when necessary.</p> <p>These Validations unveil potential gaps or missing values in your data, enabling proactive data enhancement. Like a well-oiled machine, tracking completeness validations enhances the overall functionality of your data ecosystem. Just as reliability Validations guarantee up-to-date information, completeness Validations guarantee a holistic, accurate dataset.</p>"},{"location":"validations/completeness/#null-count","title":"Null Count","text":"<p>Null count Validations gauge missing data, a crucial aspect of completeness Validations, revealing gaps and potential data quality issues.</p> <p>Example</p> dcs_config.yaml<pre><code>validations for product_db.products:\n  - null count percentage _in_dataset:\n      on: count_null(first_name)\n</code></pre>"},{"location":"validations/completeness/#null-percentage","title":"Null Percentage","text":"<p>Null percentage Validations reveal missing data, a vital facet of completeness Validations, ensuring data sets are whole and reliable.</p> <p>Example</p> dcs_config.yaml<pre><code>validations for product_db.products:\n  - empty_string_percentage_in_dataset:\n      on: percent_null(first_name)\n</code></pre>"},{"location":"validations/completeness/#empty-string","title":"Empty String","text":"<p>Empty string Validations gauge the extent of missing or null values, exposing gaps that impact data completeness and reliability.</p> <p>Example</p> dcs_config.yaml<pre><code>validations for product_db.products:\n  - empty_string_percentage_in_dataset:\n      on: count_empty_string(first_name)\n</code></pre>"},{"location":"validations/completeness/#empty-string-percentage","title":"Empty String Percentage","text":"<p>Empty String Percentage Validations assess data completeness by measuring the proportion of empty strings in datasets.</p> <p>Example</p> dcs_config.yaml<pre><code>validations for product_db.products:\n  - empty_string_percentage_in_dataset:\n      on: percent_empty_string(first_name)\n</code></pre>"},{"location":"validations/custom_sql/","title":"Custom SQL Validation","text":"<p>If the built-in set of validations does not quite give you the information you need from a Validation, you have the flexibility to define your own Validations using <code>custom_sql</code>.</p> <p>The custom SQL Validation empowers you to enter your own completely custom SQL query, providing you with the ability to create much more complex and specify monitors according to your specific requirements. This feature allows you to dig deeper into your data and extract insights that are tailored to your unique needs.</p>"},{"location":"validations/custom_sql/#example","title":"Example","text":"<pre><code>validations for mysql_db.student:\n  - custom_sql_example:\n      on: custom_sql\n      query: |\n        SELECT COUNT(*) FROM student WHERE city = 'bangalore' AND age &gt;= 30\n</code></pre>"},{"location":"validations/numeric_distribution/","title":"Numeric Distribution Validations","text":"<p>Numeric Distribution Validations detect changes in the numeric distribution of values, including outliers, variance, skew and more</p>"},{"location":"validations/numeric_distribution/#average","title":"Average","text":"<p>Average Validations gauge performance in transitional databases and search engines, offering valuable insights into overall effectiveness.</p> <p>Example</p> dcs_config.yaml<pre><code>validations for product_db.products:\n  - avg_price:\n      on: avg(price)\n      where: \"country_code = 'IN'\"\n      threshold: \"&lt; 190\"\n</code></pre>"},{"location":"validations/numeric_distribution/#minimum","title":"Minimum","text":"<p>Minimum Validations ensure consistency across transitional databases and search engines, enhancing data quality and retrieval accuracy.</p> <p>Example</p> dcs_config.yaml<pre><code>validations for product_db.products:\n  - min_price:\n      on: min(price)\n      threshold: \"&gt; 0\"\n</code></pre>"},{"location":"validations/numeric_distribution/#maximum","title":"Maximum","text":"<p>Maximum Validations gauge the highest values within datasets, helping identify outliers and understand data distribution's upper limits for quality assessment.</p> <p>Example</p> dcs_config.yaml<pre><code>validations for product_db.products:\n  - max_price:\n      on: max(price)\n      threshold: \"&lt; 1000\"\n</code></pre>"},{"location":"validations/numeric_distribution/#sum","title":"Sum","text":"<p>Sum Validations measure the total of all values within a dataset, indicating the overall size of a particular dataset to help understand data quality.</p> <p>Example</p> dcs_config.yaml<pre><code>validations for product_db.products:\n  - sum_of_price:\n      on: sum(price)\n      threshold: \"&gt; 100 &amp; &lt; 1000\"\n</code></pre>"},{"location":"validations/numeric_distribution/#variance","title":"Variance","text":"<p>Variance in data quality measures the degree of variability or dispersion in a dataset, indicating how spread out the data points are from the mean.</p> <p>Example</p> dcs_config.yaml<pre><code>validations for product_db.products:\n  - variance_of_price:\n      on: variance(price)\n      threshold: \"&lt; 2.0\"\n</code></pre>"},{"location":"validations/numeric_distribution/#standard-deviation","title":"Standard Deviation","text":"<p>Standard deviation Validations measure the amount of variation or dispersion of a set of values from the mean, indicating how spread out the data points are from the mean.</p> <p>Example</p> dcs_config.yaml<pre><code>validations for product_db.products:\n  - standard_deviation_of_price:\n      on: stddev(price)\n      threshold: \"&lt; .81\"\n</code></pre>"},{"location":"validations/reliability/","title":"Reliability Validations","text":"<p>Reliability Validations are an essential tool for ensuring that your tables, indices, or collections are being updated with the most up-to-date and timely data.</p> <p>By consistently monitoring these validations, you can gain better insights into how your systems are performing and make more informed decisions about how to optimize and improve performance. Additionally, these validations can help you identify any potential issues or bottlenecks in your data pipelines, allowing you to take proactive steps to address them before they become major problems.</p> <p>Overall, investing in a reliable and robust set of validations is crucial for maintaining the health and performance of your data applications and ensuring that your systems are running as smoothly and efficiently as possible.</p>"},{"location":"validations/reliability/#freshness","title":"Freshness","text":"<p>Data freshness, also known as data timeliness, refers to the frequency at which data is updated for consumption. It is an important dimension of data quality and a pillar of data observability because recently updated data is more accurate and, therefore, more valuable.</p> <p>In the below example the validation will look for the last updated timestamp of the table or index using <code>updated_at</code> field.</p> <p>The threshold will trigger a validation error when the validation is greater than 86400 seconds</p> <p>Example</p> dcs_config.yaml<pre><code>validations for product_db.products:\n  - freshness_of_products:\n      on: freshness(updated_at)\n      threshold: \"&gt; 86400\"\n</code></pre>"},{"location":"validations/reliability/#row-count","title":"Row Count","text":"<p>The row count validation determines the total number of rows present in a table.</p> <p>Example</p> dcs_config.yaml<pre><code>validations for product_db.products:\n  - count of products:\n      on: count_rows\n      where: \"country_code = 'IN'\"\n      threshold: \"&gt; 1000\"\n</code></pre>"},{"location":"validations/reliability/#document-count","title":"Document Count","text":"<p>The document count validation determines the total number of documents present in a search data source index.</p> <p>Example</p> dcs_config.yaml<pre><code>validations for search_datastore.product_data_index:\n  - count of documents:\n      on: count_documents\n      threshold: \"&gt; 1000\"\n</code></pre>"},{"location":"validations/uniqueness/","title":"Uniqueness Validations","text":"<p>Uniqueness Validations play a pivotal role in upholding data quality standards. Just as reliability Validations ensure timely data updates, uniqueness Validations focus on the distinctiveness of data entries within a dataset.</p> <p>By consistently tracking these Validations, you gain valuable insights into data duplication, redundancy, and accuracy. This knowledge empowers data professionals to make well-informed decisions about data cleansing and optimization strategies. Uniqueness Validations also serve as a radar for potential data quality issues, enabling proactive intervention to prevent major problems down the line.</p>"},{"location":"validations/uniqueness/#distinct-count","title":"Distinct Count","text":"<p>A distinct count Validation in data quality measures the number of unique values within a dataset, ensuring accuracy and completeness.</p> <p>Example</p> dcs_config.yaml<pre><code>validations for product_db.products:\n  - distinct_count_of_product_categories:\n      on: count_distinct(product_category)\n</code></pre>"},{"location":"validations/uniqueness/#duplicate-count","title":"Duplicate Count","text":"<p>Duplicate count is a data quality Validation that measures the number of identical or highly similar records in a dataset, highlighting potential data redundancy or errors.</p> <p>Example</p> dcs_config.yaml<pre><code>validations for product_db.products:\n  - distinct count of product categories:\n      on: count_duplicate(product_category)\n</code></pre>"},{"location":"validations/validity/","title":"Validity Validations","text":"<p>Validity checks ensure data is not only correctly formatted but also valid. These metrics are crucial for data quality assurance by verifying adherence to predefined rules and standards. Implementing these checks helps users detect and fix errors, maintaining data integrity and reliability.</p>"},{"location":"validations/validity/#count-uuid","title":"Count UUID","text":"<p>The count UUID validation checks the number of UUIDs in a dataset.</p> <p>Example</p> dcs_config.yaml<pre><code>validations for product_db.products:\n  - count uuid for product_id:\n      on: count_uuid(product_id)\n      threshold: \"&gt; 100\"\n</code></pre>"},{"location":"validations/validity/#percentage-uuid","title":"Percentage UUID","text":"<p>The percentage UUID validation checks the percentage of UUIDs in a dataset.</p> <p>Example</p> dcs_config.yaml<pre><code>validations for product_db.products:\n  - percentage uuid for product_id:\n      on: percent_uuid(product_id)\n      threshold: \"&gt; 90\"\n</code></pre>"}]}